{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandr/Desktop/mobile/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA_FOLDER = \"vkcup2022-first-stage/\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(PATH_TO_DATA_FOLDER,'train.csv'))\n",
    "test = pd.read_csv(os.path.join(PATH_TO_DATA_FOLDER,'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U transformers==4.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model = \"ai-forever/ruRoberta-large\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(raw_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_scen = dict(zip(sorted(data['sm'].unique().tolist()),range(len(data.sm))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data['scenario_id'].map(dict(zip(sorted(data['scenario_id'].unique().tolist()),range(1,len(topics))))).isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_scen = dict(**map_scen, **{'Сбермобайл. Wi Fi звонки': 28})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sm'].map(map_scen).isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df):\n",
    "    df['is_fake'] = df['sm'].map(map_scen)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairsDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "def data_collator(batch):\n",
    "    y = torch.Tensor([p[1] for p in batch]).to(model.device)\n",
    "    x = tokenizer([p[0] for p in batch], return_tensors='pt', padding=True).to(model.device)\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.sm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = transform_df(data)\n",
    "test = transform_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(train, valid):\n",
    "    train_dataset = PairsDataset(train.text.values, train.is_fake.values)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
    "    \n",
    "    valid_dataset = PairsDataset(valid.text.values, valid.is_fake.values)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, drop_last=False, shuffle=False, collate_fn=data_collator)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    num = 0\n",
    "    den = 0\n",
    "    y_true = list()\n",
    "    y_pred = list()\n",
    "    y_pred_prob = list()\n",
    "    f1_valid = .0\n",
    "    for x, y in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                input_ids=x.input_ids,\n",
    "                attention_mask=x.attention_mask,\n",
    "                # labels=y,\n",
    "                return_dict=True\n",
    "            )\n",
    "            loss = F.cross_entropy(output.logits, y.long())\n",
    "            # loss = output.loss\n",
    "            \n",
    "            num += len(x) * loss.item()\n",
    "            den += len(x)\n",
    "            \n",
    "            y_pred.extend(torch.argmax(output.logits, 1).tolist())\n",
    "            y_pred_prob.extend(softmax(output.logits, dim = 1)[:, 1].tolist())\n",
    "            y_true.extend(torch.argmax(y.unsqueeze(1), 1).tolist())\n",
    "            \n",
    "    val_loss = num / den\n",
    "    f1_valid = f1_score(y_true, y_pred, average = 'micro')\n",
    "    return val_loss, f1_valid, y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(map_scen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    model, train_dataloader, val_dataloader, \n",
    "    max_epochs=10, \n",
    "    lr=1e-5,\n",
    "    eval_steps = 50\n",
    "):\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size = 3, gamma=0.5)\n",
    "    best_f1 = float('-inf')\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        print('EPOCH', epoch)\n",
    "        losses = list()\n",
    "        for i, (x, y) in enumerate(train_dataloader):\n",
    "            output = model(\n",
    "                input_ids=x.input_ids,\n",
    "                attention_mask=x.attention_mask,\n",
    "                # labels=y,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            # class_indices = torch.argmax(y, dim=1)\n",
    "            loss = F.cross_entropy(output.logits,  y.long())\n",
    "            # loss = output.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            if i % eval_steps == 0:\n",
    "                model.eval()\n",
    "                train_loss = np.mean(losses[-eval_steps:])\n",
    "                eval_loss, eval_f1, _ = evaluate_model(model, val_dataloader)\n",
    "                if eval_f1 > best_f1:\n",
    "                    best_f1 = eval_f1\n",
    "                    torch.save(model.state_dict(), SAVE_PATH)\n",
    "                print(f'step {i} train_loss: {train_loss:.3} eval_loss: {eval_loss:.3} eval_f1: {eval_f1:.3}')\n",
    "                model.train()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====  FOLD 0  =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "step 0 train_loss: 3.67 eval_loss: 3.02 eval_f1: 0.397\n",
      "step 50 train_loss: 1.61 eval_loss: 1.18 eval_f1: 0.398\n",
      "step 100 train_loss: 1.14 eval_loss: 0.875 eval_f1: 0.382\n",
      "step 150 train_loss: 0.855 eval_loss: 0.717 eval_f1: 0.385\n",
      "step 200 train_loss: 0.718 eval_loss: 0.631 eval_f1: 0.381\n",
      "step 250 train_loss: 0.641 eval_loss: 0.563 eval_f1: 0.384\n",
      "step 300 train_loss: 0.538 eval_loss: 0.528 eval_f1: 0.383\n",
      "step 350 train_loss: 0.512 eval_loss: 0.497 eval_f1: 0.379\n",
      "step 400 train_loss: 0.511 eval_loss: 0.473 eval_f1: 0.384\n",
      "step 450 train_loss: 0.486 eval_loss: 0.423 eval_f1: 0.384\n",
      "step 500 train_loss: 0.448 eval_loss: 0.407 eval_f1: 0.383\n",
      "step 550 train_loss: 0.336 eval_loss: 0.38 eval_f1: 0.385\n",
      "step 600 train_loss: 0.486 eval_loss: 0.348 eval_f1: 0.381\n",
      "step 650 train_loss: 0.341 eval_loss: 0.325 eval_f1: 0.382\n",
      "step 700 train_loss: 0.389 eval_loss: 0.354 eval_f1: 0.383\n"
     ]
    }
   ],
   "source": [
    "N_SPLITS = 2\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1\n",
    "SAVE_PATH = 'ruroberta_model'\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "test_results = list()\n",
    "\n",
    "test_dataset = PairsDataset(test.text.values, test.is_fake.values)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, drop_last=False, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "    print(f\"=====  FOLD {i}  =====\")\n",
    "    train, valid = data.iloc[train_index], data.iloc[test_index]\n",
    "    train_dataloader, valid_dataloader = get_dataloaders(train, valid)\n",
    "    \n",
    "    model = RobertaForSequenceClassification.from_pretrained(raw_model, num_labels = num_classes);\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    train_loop(model, train_dataloader, valid_dataloader, max_epochs=EPOCHS, lr=2e-5, eval_steps = 50)\n",
    "    \n",
    "    model.load_state_dict(torch.load(SAVE_PATH))\n",
    "    model.eval()\n",
    "    \n",
    "    _, _, test_probability = evaluate_model(model, test_dataloader)\n",
    "    test_results.append(test_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-26T12:54:09.044343Z",
     "iopub.status.busy": "2023-11-26T12:54:09.044016Z",
     "iopub.status.idle": "2023-11-26T12:54:09.063402Z",
     "shell.execute_reply": "2023-11-26T12:54:09.062493Z",
     "shell.execute_reply.started": "2023-11-26T12:54:09.044305Z"
    }
   },
   "outputs": [],
   "source": [
    "assert all([len(x) == len(test) for x in test_results])\n",
    "\n",
    "predictions = np.mean(test_results, axis = 0)\n",
    "test.is_fake = [1 if x >= 0.5 else 0 for x in predictions]\n",
    "test.to_csv('predictions.tsv', index = False, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aleksandr/Desktop/mobile\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2056324,
     "sourceId": 3411463,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30179,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
